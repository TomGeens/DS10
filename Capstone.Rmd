---
title: "Data Science Capstone"
author: "Tom Geens"
date: "8 maart 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Background information and getting the data
Background information for the project  
* Text Mining Infrastructure in R ([download pdf](https://www.jstatsoft.org/index.php/jss/article/view/v025i05/v25i05.pdf))  
* [CRAN Task View NLP](https://cran.r-project.org/web/views/NaturalLanguageProcessing.html)  
*  [Natural Language Processing Wikipedia entry](https://en.wikipedia.org/wiki/Natural_language_processing)  


The dataset for the project has to be downloaded [here](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip).

```{r, cache=TRUE}
if(!file.exists("ds.zip")){
  download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip","ds.zip")
  unzip("ds.zip")
  }
if(!file.exists("tmir.pdf")){download.file("https://www.jstatsoft.org/index.php/jss/article/view/v025i05/v25i05.pdf","tmir.pdf")}
if(!file.exists("nlp.pdf")){download.file("https://en.wikipedia.org/w/index.php?title=Special:Book&bookcmd=download&collection_id=eb7e52dcdd3e8268168a3211ec33a5bdf2f71239&writer=rdf2latex&return_to=Natural+language+processing","nlp.pdf")}
```


The first step in analyzing our new data set is figuring out:  
1. what data you have and  
2. what are the standard tools and models used for that type of data. 

This exercise uses the files named LOCALE.blogs.txt where LOCALE is the each of the four locales en\_US, de\_DE, ru\_RU and fi\_FI. The data is from a corpus called HC Corpora [www.corpora.heliohost.org](www.corpora.heliohost.org). See the readme file at [http://www.corpora.heliohost.org/aboutcorpus.html](http://www.corpora.heliohost.org/aboutcorpus.html) for details on the corpora available. The files have been language filtered but may still contain some foreign text.

For example, the following code could be used to read the first few lines of the English Twitter dataset:

```{r}
con <- file("en_US.twitter.txt", "r") 
readLines(con, 1) ## Read the first line of text 
readLines(con, 1) ## Read the next line of text 
readLines(con, 5) ## Read in the next 5 lines of text 
close(con) ## It's important to close the connection when you are done
```

You might want to create a separate sub-sample dataset by reading in a random subset of the original data and writing it out to a separate file. That way, you can store the sample and not have to recreate it every time. You can use the rbinom function to "flip a biased coin" to determine whether you sample a line of text or not.

```{r}
file.size("final/en_US/en_US.blogs.txt") ## show file size 
```

```{r}
con <- file("final/en_US/en_US.twitter.txt", "r") 
length(readLines(con, -1)) ## count the number of lines in twitter 
close(con)
```

```{r}
con <- file("final/en_US/en_US.blogs.txt", "r") 
blogs<-readLines(con, -1) ## count the number of lines in blogs 
close(con)
b<-max(stringi::stri_length(blogs))

con <- file("final/en_US/en_US.news.txt", "r") 
news<-readLines(con, -1) ## count the number of lines in news 
close(con)
n<-max(stringi::stri_length(news))

con <- file("final/en_US/en_US.twitter.txt", "r") 
twitter<-readLines(con, -1) ## count the number of lines in twitter 
close(con)
t<-max(stringi::stri_length(twitter))
rm(con)

max(b,n,t)
```

```{r}
length(grep("love",twitter))/length(grep("hate",twitter))
twitter[grep("biostats",twitter)]
length(grep("A computer once beat me at chess, but it was no match for me at kickboxing",twitter))
```

#tasks 1
Tokenization - identifying appropriate tokens such as words, punctuation, and numbers. Writing a function that takes a file as input and returns a tokenized version of it.
Profanity filtering - removing profanity and other words you do not want to predict.

#task 2
Exploratory analysis - perform a thorough exploratory analysis of the data, understanding the distribution of words and relationship between the words in the corpora.
Understand frequencies of words and word pairs - build figures and tables to understand variation in the frequencies of words and word pairs in the data.

#task 3
Build basic n-gram model - using the exploratory analysis you performed, build a basic n-gram model for predicting the next word based on the previous 1, 2, or 3 words.
Build a model to handle unseen n-grams - in some cases people will want to type a combination of words that does not appear in the corpora. Build a model to handle cases where a particular n-gram isn't observed.

#milestone report
The goal of this project is just to display that you've gotten used to working with the data and that you are on track to create your prediction algorithm. Please submit a report on R Pubs [http://rpubs.com/](http://rpubs.com/) that explains your exploratory analysis and your goals for the eventual app and algorithm. This document should be concise and explain only the major features of the data you have identified and briefly summarize your plans for creating the prediction algorithm and Shiny app in a way that would be understandable to a non-data scientist manager. You should make use of tables and plots to illustrate important summaries of the data set. The motivation for this project is to:  
1. Demonstrate that you've downloaded the data and have successfully loaded it in.  
2. Create a basic report of summary statistics about the data sets.  
3. Report any interesting findings that you amassed so far.  
4. Get feedback on your plans for creating a prediction algorithm and Shiny app.

Review criteria  
* Does the link lead to an HTML page describing the exploratory analysis of the training data set?  
* Has the data scientist done basic summaries of the three files? Word counts, line counts and basic data tables?  
* Has the data scientist made basic plots, such as histograms to illustrate features of the data?
*  Was the report written in a brief, concise style, in a way that a non-data scientist manager could appreciate?